An accuracy of a parametric model depends on the data provided to it and  parameters used. The same holds for neural networks. The parameters in the neural networks are weights and during the training we are trying to find \textit{the best} weights. To find them, we express how bad the model is using the \textit{loss function} or \textit{cost function}. It expresses how much wrong is the model (for a given data) using the given weights. When such a function is defined, all we do is try to find an input to that function i.e. weights, such that output of that function i.e. loss, is minimal. Most of the machine learning algorithms have some kind of an optimization in it. Optimization refers to the task of find $\pmb x$ s.t. $f(\pmb x)$ is maximal or minimal. In this thesis, optimization will always mean minimization, except when stated otherwise. Maximization can be accomplished by minimizing the function $-f(\pmb x)$.

Gradient descent is 
%the main idea behind the famous backpropagation algorithm, which is described in the Subsection \ref{subsection:backpropagation}. 