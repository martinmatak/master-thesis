An accuracy of a parametric model depends on the data provided to it and  parameters used. The same holds for neural networks. The parameters in the neural networks are weights and during the training we are trying to find \textit{the best} weights. To find them, we express how bad the model is using the \textit{loss function} or \textit{cost function}. It expresses how much wrong is the model (for a given data) using the given weights. When such a function is defined, all we do is try to find an input to that function i.e. weights, such that output of that function i.e. loss, is minimal. Most of the machine learning algorithms have some kind of an optimization in it. Optimization refers to the task of find $\pmb x$ s.t. $f(\pmb x)$ is maximal or minimal. In this thesis, optimization will always mean minimization, except when stated otherwise. Maximization can be accomplished by minimizing the function $-f(\pmb x)$.

Since the main idea behind training the neural network is to minimize a loss function, two things need to be decided. The first thing is which loss function should be used and the second thing is which optimization algorithm will be used to find its minimum.

Most popular classification loss is \textit{cross-entropy}. Given two probability mass functions (a mass function is a function that gives the probability that a discrete random variable is exactly equal to some value) $\pmb u$ and $\pmb v$ in $\mathbb{R}^T$, i.e. $\pmb u = (u_1, ..., u_T)$ and $\pmb v =  (v1_, ..., v_T)$, the cross-entropy between $\pmb u$ and $\pmb v$ is
 \begin{equation}
H (\pmb u, \pmb v)= - \sum_{t=1}^T  u_t  \text{ln} v_t 
\end{equation}

To use cross-entropy for training a classifier, let $\pmb u$ enocde the ground-truth label, $u_c = 1$ where $c$ is index of the class. Let $\pmb v$ be the predicted softmax class scores. Now $H$ measures how dissimilar true and predicted probabilities are for a single sample.

Let $\mathcal{D}$ be a dataset such that
$ \mathcal{D} = \{ (\pmb x_s, \pmb w_s)\}_{s=1}^{S}$ where $x_s$ is an input vector and $w_s$ is a vector encoding the ground truth (i.e. all indices have value $0$, except on the index of the class which $x_s$ represents where is value $1$).

On this basis, \textit{cross-entropy loss} on $\mathcal{D}$ is defined as
\begin{equation} \label{eq:loss-function}
L(\pmb \theta) = \frac{1}{S} \sum_{s=1}^{S} H (\pmb w_s, \text{softmax} (f(\pmb x_s ; \pmb \theta)))
\end{equation}

Models trained with this loss are called \textit{softmax classifiers}. If $T = 2$, it is also called \textit{logistic regression}. Classifiers learn to predict probabilities per class label.

After we defined loss function $L(\pmb \theta)$  as in \ref{eq:loss-function}, we need to find an optimization algorithm to find its minimum. Since the function is not linear in $\pmb \theta$, a nonlinear optimization algorithm is needed. A popular choice in deep learning is \textit{Gradient Descent}

Gradient Descent is an iterative optimization algorithm that is used for finding the minimum of a function. It is based on the first derivative of the function which minimum needs to be found. This means that the function must be differentiable. 

Let $\nabla f$ be a vector of all partial derivatives of a function $f$. Then a gradient descent is shown in the Algorithm \ref{alg:gradient-descent}.

\begin{algorithm}[htb]
\caption{Gradient descent implementation}
\label{alg:gradient-descent}

\KwIn{$\pmb \theta, L, \alpha $}

\While {$true$}{
    $\pmb \theta^{'} \leftarrow \nabla L(\pmb \theta)$\;
    \If {$\| \pmb \theta^{'}\|  \approx 0 $} {
    		\Return\;
    }
    $\pmb \theta \leftarrow \pmb \theta - \alpha * \pmb \theta^{'}$\;
}
\end{algorithm}


TODO: Stochasting gradient descent, momentum, nesterov momentum
 An interested reader can find an overview of gradient descent optimization algorithms in \ref{gradient-descent-overview}. 



