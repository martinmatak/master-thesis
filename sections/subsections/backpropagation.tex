Now we already know how to train a neural network. We can use a cross-entropy loss function $L (\pmb \theta)$ and minibatch gradient descent with (Nesterov) momentum. For gradient descent, we must calculate $\nabla L (\pmb \theta)$.

One way to obtain $\nabla L (\pmb \theta)$ is \textit{numerical differentiation}. 
Let $p \in [1, dim(\pmb \theta)]$, $\epsilon \in \mathbb{R}$ be arbitrary small and $\pmb 1_p$ be a vector which is $1$ at position $p$ and $0$ otherwise. Then directly by definition of the derivative we obtain \ref{eq:numerical-derivative-1}

\begin{equation}\label{eq:numerical-derivative-1}
\nabla L (\pmb \theta) = (L(\pmb \theta + \pmb 1_p \epsilon) - L (\pmb \theta)) / \epsilon
\end{equation}

Sometimes is \ref{eq:numerical-derivative-2} used instead of \ref{eq:numerical-derivative-1}.
\begin{equation}\label{eq:numerical-derivative-2}
\nabla L (\pmb \theta) = (L(\pmb \theta + \pmb 1_p \epsilon) - L (\pmb \theta - \pmb 1_p \epsilon)) / 2 \epsilon
\end{equation}

This approach is easy to implement, but it's only an approximation ($\epsilon$ cannot be arbitrary small) and it is too inefficient in practice because $L$ must be evaluated $dim(\pmb\theta)$ times (modern DNNs have millions of parameters). Instead of \textit{numerical differentiation}, preferable way is to obtain $\textit{analytic grdient}$, i.e. obtain $\nabla L (\pmb \theta)$ analytically using calculus. It's more accurate (no approximation) and much more efficient (single evaluation).


Neural network is computational graph composed of other functions. Loss function $L$ of a neural network is again a graph. Derivatives in such graphs can be computed iteratively using the chain rule \ref{eq:chain-rule}. 

Let $f: \mathbb{R} \mapsto \mathbb{R}$, $g: \mathbb{R} \mapsto \mathbb{R}$ and $F: \mathbb{R} \mapsto \mathbb{R}$ s.t. $F(x) = f(g(x))$. Then by the chain rule, $F'(x) = f'(g(x))g'(x)$.

Based on the chain rule, gradient of every weight can be efficiently computed and then updated as defined in gradient descent method. The paper in which this famous algorithm is introduced is \cite{Rumelhart:1986:LIR:104279.104293}.