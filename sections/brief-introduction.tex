Formal definition of a machine learning algorithm is given by \cite{Mitchell:1997:ML:541177}: "A computer program is said to learn from experience $E$ with respect to some class of tasks $T$ and performance measure $P$ if its performance at tasks in $T$, as measured by $P$, improves with experience $E$." 
From such a vague definition, it is obvious that complete introduction to machine learning is too broad topic and hence in this thesis background on the \textit{image classification} task is provided only. That should be enough for a non-expert to be able to read and to understand this whole thesis.

As explained in Section \ref{motivation}, an image classification is a task of assigning a \textit{label} or a \textit{class} to an image. If an input is $n$-dimensional vector and there are $k$ classes, then the learning algorithm is usually asked to produce a function $f: \mathbb{R}^n \rightarrow \{1, ... , k\}$. One other variant of a classification task would be to produce a function $f$ which outputs a probability distribution over classes, i.e. how likely each class is. In such a scenario, next step usually is to assign a label according to the most likely class, but it must not be the case. A model that is used for classification task is called a \textit{classifier}.

After defining a task, it is important to know how to measure the performance of a model. Depending on the domain of a system, this measure can vary. However, for a classification task, the \textit{accuracy} of the model is usually measured. It is the fraction of correct predictions of the model. For example, if the model was correct for 8 out of 10 samples, the accuracy of that model (on those 10 samples) is $0.8$ or $80\%$. An equivalent information can be obtained by measuring the \textit{error rate} which is defined as the fraction of incorrect predictions of the model. 

Usually we are interested how good the model is on previously unseen data, because that way we can see how good will it work in the real world application. Therefore, we measure its performance on a set of data which is not used during training. Such a set of data is called \textit{test set}. A test set is subset of an entire \textit{dataset}.

A dataset is a collection of samples. One of the oldest datasets is the Iris flower dataset \cite{iris-dataset}. The dataset consists of 50 samples from each of three species of Iris. For each sample, four measures are taken: the length and the width of the sepals and petals, in centimeter. A measurable property of a sample is called a \textit{feature}. Hence, Iris dataset has 3 classes, 150 samples and each sample has 4 features. A classifier for that dataset could be modelled as a function $f: \mathbb{R}^4 \rightarrow \{0, 1, 2\}$ where $0, 1, 2$  encode the first, the second and the third Iris type, respectfully.

Most machine learning algorithms have \textit{hyperparameters}, settings that we can use to control the algorithms' behaviour. Hyperparameters are values which are set before the learning algorithm begins. By contrast, the values of other parameters are derived via training. 

If a classifier has a high accuracy on a test dataset, we say it \textit{generalizes} well. However, if it has a high accuracy on the training samples, but a low accuracy on a test dataset, we say it \textit{overfits}. To avoid overfitting, training samples are split into two nonoverlapping subsets: \textit{training} subset and \textit{validation} subset.

Validation subset is also used to find good hyperparameters. Training subset is used to adjust parameters of a model. Validation subset is only verifying that any increase in accuracy over the training dataset actually yields an increase in accuracy over a dataset that has not been shown to the model before, or at least the model hasn't trained on it. If the accuracy over the training dataset increases, but the accuracy over then validation dataset stays the same or decreases, then a model is overfitting. Test accuracy, the accuracy we are typically interested in, is computed on a test dataset. That means that an original dataset is split into three disjoint subsets: training, validation and test subset. 

A problem can occur if there is not enough data, i.e. an original dataset is small. Then training, validation and test subsets are not big enough. For instance, it can happen that training dataset doesn't have any instance of some specific class or there is too few of them. That problem can be solved by the \textit{k-fold cross-validation} procedure.

Such a procedure is based on the idea of repeating the training and testing computation on different randomly chosen subsets or splits of the original dataset. The original set is split into $k$ equal disjoint subsets. Of the $k$ subsets, a single subset is retained as the validation set for testing the model, and the remaining $k-1$ subsets are used as training data. The process is then repeated $k$ times, with each of the $k$ subsets used exactly once as the validation data. The $k$ results can then be averaged to produce a test accuracy. 

  

