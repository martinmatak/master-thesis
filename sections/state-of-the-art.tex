Machine learning is a field which is being fast developed and therefore a lot of papers are published in the last few years. However, since focus of this master thesis is on generating adversarial examples, related work can be separated in two topics, depending if a DNN is treated as a white-box or a black-box:

In terms of white-box attacks, in \cite{fgsm-original}, \textit{Fast Gradient Sign Method} is presented - a method which computes an adversarial image for a non-targeted attack based on the direction of the gradient of a DNN. In this thesis, a targeted version \cite{fgsm-targeted} of it will be used. The target label in the paper was always a label with the least probability for an unmodified image. They evaluate their method on ImageNet dataset \cite{datasetImageNet}, a large-image recognition task with 1000 classes. Since goal in the paper was a misclassification, results for a targeted misclassification are not presented. 

In \cite{DBLP:journals/corr/PapernotMJFCS15}, JSMA algorithm for generating adversarial examples is presented. It is based on identifying regions in an image which have higher importance for a DNN during the classification. \textit{Adversarial Saliency Maps} - maps which measure how much is every pixel important for an image to be classified as a specific class - are created. Based on them and \textit{forward derivative} of the DNN, adversarial examples are crafted. The algorithm is validated against MNIST dataset \cite{datasetMNIST}, a digit-recognition task (0-9). The result is that using JSMA algorithm, an attacker is able to craft a successful adversarial sample for every class.

In \cite{DBLP:journals/corr/CarliniW16a}, CW attack is presented which is based on formulating the attack as an optimization problem and using a state-of-the-art optimizer to solve it. According to authors, this attack is often much more effective (and never worse) than all the others presented in the literature. 
Attacks are evaluated on three datasets: ImageNet, MNIST and CIFAR-10 \cite{datasetCIFAR10}, a small-image recognition task, also with 10 classes as in MNIST. They also report that JSMA is always failing on ImageNet dataset due to memory complexity of the algorithm, i.e. ImageNet dimensions of an image (229x229x3) are too big for JSMA. This implies that JSMA could not work in my thesis as well if an image of a person is too big. Reported results for CW attack are showing 100\% success against all three datasets.

All three attacks, FGSM, JSMA and CW will be used and explained in this thesis.

On the black-box side of the attacks, there is \textit{transfer-based} approach 
\cite{DBLP:journals/corr/PapernotMGJCS16}. It uses a subsitute DNN which is trained on a similar dataset as the targeted DNN. Adversarial images are crafted then for a substitute DNN using a white-box approach. Those images are used then as adversarial images for a black-box DNN as well.
Authors present results for misclassification attack against MNIST dataset and GTSRD dataset \cite{datasetGTSRD}. 

In \cite{ensemble-attack}, authors show that adversarial samples for targeted misclassification don't transfer that good as in pure misclassification attack. Authors suggest \textit{ensemble} approach. This approach is also based on transferability of an adversarial image, but instead of generating an adversarial image for one neural network, generates it for several of them. The underlying assumption is that if an adversarial example works as expected among several models, it will work as expected for the one more as well. Both approaches will be implemented - when a substitute network is only a single neural network, and when there is several of them. I want to see if it's enough to use only one neural network as a substitute to achieve good results in semi-targeted misclassification.

In \cite{brendel2018decisionbased}, authors implement a completely different attack and call it \textit{Boundary Attack}. The attack starts with an image of a targeted class and then, step by step, it changes it to an image of some other class while staying adversarial, i.e. classified as a target class by a DNN under the attack. In every iteration of the attack, the image is changed a bit towards a class which will be in the image in the end, according to a human observer. After every change, the DNN is queried to check if the image is still adversarial, i.e. classified as a target label. If not, a change is reverted. In this way, they don't need any substitute neural network, but they do need a lot of queries to the targeted DNN. Authors compare this attack with CW attack on MNIST and CIFAR-10 and produce only a bit worse results, although this attack is treating a DNN as a black-box. This approach will be compared with the \textit{ensemble} approach in this thesis.