As Artificial Intelligence (AI) is getting a greater impact in our everyday life, it is of utmost importance that it is safe for humans.  

One of the tools used in AI are deep neural networks (DNNs) and neural networks in general. Deep neural networks are powerful learning models that achieve excellent performance on visual recognition problems \cite{krizhevsky2012imagenet}. Those results imply that DNNs can be used in different industry domains, e.g. traffic sign recognition, a domain in which DNNs even outperform humans \cite{outperformhumans}. 

Nevertheless, neural networks tend to have some peculiar properties as well. If only several pixels in an image are changed, some neural networks produce incorrect results \cite{szegedy2013intriguing}. Such a behaviour is not allowed in safety-critical systems. For example, if an autonomous car recognizes a  \textit{STOP} sign as anything else but a \textit{STOP} sign, it can lead to deathly consequences. Therefore, it is important to verify the system. 

The assumption in the rest of this proposal is that the neural network is performing a \textit{classification task} - mapping an input (image) to a discrete output value (e.g. mapping an image of a traffic sign to the name of the traffic sign).  Output values are often called labels or categories. There is a finite number of possible labels.

To find errors in a system where a neural network is used, we need a framework which can trigger every possible output of the neural network.  We want to achieve that by using only one image as a starting point. 

Since for the same input we will always get the same output, modifications of that image are necessary. For a different desired output, a different modification on the image is performed.
Then we repeat that process by changing our desired output in every iteration and in that way, cover all possible outputs. Of course, the idea is that the modified image is as close as possible to the original one. For example, an image of a  \textit{STOP} sign can be modified as long as it still is an image of a  \textit{STOP} sign to a human observer. In that case, with the variations of the \textit{STOP} sign we cover all the possible outputs of the neural network - all other traffic signs. Using this technique, we can trigger errors in the system which can help us in the process of its verification. 

Modifying an input for a neural network with a goal of reaching an output that is different than the output of an unmodified input is an attack called \textit{misclassification}. If an output which an attacker wants to reach is one specific label, then a name of the attack is \textit{targeted misclassification}. Creating a framework for targeted misclassification in the domain of age estimation as well as comparison and evaluation of existing approaches is the main focus of this thesis.