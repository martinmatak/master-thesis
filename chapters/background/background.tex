A formal definition of a machine learning algorithm is given by \cite{Mitchell:1997:ML:541177}: "A computer program is said to learn from experience $E$ with respect to some class of tasks $T$ and performance measure $P$ if its performance at tasks in $T$, as measured by $P$, improves with experience $E$." 
From such a vague definition, it is obvious that a comprehensive introduction to machine learning is a broad topic. Hence, in this thesis a brief introduction to the \textit{image classification} problem is provided only. That should be enough for a non-expert to be able to read and to understand this whole thesis.

As explained in Section \ref{motivation}, an image classification is a \textit{task} of assigning a \textit{label} or a \textit{class} to an image. If an input is $n$-dimensional vector and there are $k$ classes, then the learning algorithm is usually asked to produce a function $f: \mathbb{R}^n \rightarrow \{1, ... , k\}$. One other variant of a classification task would be to produce a function $f$ which outputs a probability distribution over classes, i.e. how likely each class is. In such a scenario, the next step usually is to assign a label according to the most likely class, but this must not be the case. A model that is used for classification is called a \textit{classifier}.

After defining a task, it is important to know how to measure the performance of a model. Depending on the domain of a system, this measure can vary. The \textit{accuracy} of the model is usually measured for a classification task. It is the fraction of correct predictions of the model. For example, if the prediction of the model was correct for 8 out of 10 samples, the accuracy of that model (on those 10 samples) is $0.8$ or $80\%$. An equivalent information can be obtained by measuring the \textit{error rate}. It is defined as the fraction of incorrect predictions of the model. 

Usually we are interested how good the model is on previously unseen data, because that way we can see how good will it work in the real world application. Therefore, we measure its performance on a set of data that is not used during training. Such a set of data is called \textit{test set}. A test set is subset of an entire \textit{dataset}.

A dataset is a collection of samples. One of the oldest datasets used in Machine Learning is the Iris flower dataset \cite{iris-dataset}. That dataset consists of 50 samples from each of three species of Iris. For each sample, four measures are taken: the length and the width of the sepals and petals, in centimeters. A measurable property of a sample is called a \textit{feature}. Hence, Iris dataset has 3 classes, 150 samples and each sample has 4 features. A classifier for that dataset could be modelled as a function $f: \mathbb{R}^4 \rightarrow \{0, 1, 2\}$ where $0, 1, 2$  encode the first, the second and the third type of Iris, respectfully.

For every sample in a dataset, an information is provided that defines what is a class of that sample. That information is called \textit{ground-truth label}. It is usually used as a vector. Assume there are $N$ different classes in a dataset. Then every class can be indexed from $0$ to $N-1$. Let there be a sample such that its ground-truth label has the index $p$. Then a ground-truth vector $\pmb 1_p$ represents a vector with dimensionality $N$ and all the values are $0$, except for the index $p$ where the value is $1$. In other words, a vector $\pmb 1_p$ encodes a ground-truth label.

Most machine learning algorithms have \textit{hyperparameters}, settings that we can use to control the algorithms' behaviour. Hyperparameters are values which are set before the learning algorithm begins. In contrast, the values of other parameters are derived via training. 

If a classifier has a high accuracy on a test dataset, we say it \textit{generalizes} well. However, if it has a high accuracy on the training samples, but a low accuracy on a test dataset, we say it \textit{overfits}. To avoid overfitting, training samples are split into two non overlapping subsets: the \textit{training} subset and the \textit{validation} subset.

The validation subset can be used to find good hyperparameters. The training subset is used to adjust parameters of a model. The validation subset is verifying that any increase in accuracy over the training dataset actually yields an increase in accuracy over a dataset that has not been shown to the model before, or at least the model hasn't been trained on it. If the accuracy over the training dataset increases, but the accuracy over the validation dataset stays the same or decreases, then a model is overfitting. The test accuracy, an accuracy we are typically interested in, is computed on the test dataset. That means that an original dataset is split into three disjoint subsets: training, validation and test subset. 

A problem can occur if there is not enough data, i.e. an original dataset is too small. That makes the training, the validation and the test subset not big enough. For instance, it can happen that the training dataset doesn't have any instance of some specific class or there is too few of them. That problem can be solved by the \textit{k-fold cross-validation} procedure.

That procedure is based on the idea of repeating the training and testing computation on different splits of the original dataset. The original set is split into $k$ equal disjoint subsets. Of the $k$ subsets, a single subset is retained as the validation set for testing the model, and the remaining $k-1$ subsets are used as training data. The process is then repeated $k$ times, with each of the $k$ subsets used exactly once as the validation data. The $k$ results can then be averaged to produce a test accuracy. 

So far, the basics of the machine learning are covered. A goal for the rest of this chapter is to provide a brief introduction to the deep learning area. In Section \ref{subsection:FNN} an introduction to the basic neural networks is provided. After that, in Section \ref{subsection:gradient-descent} the Gradient Descent algorithm and variations of it are introduced. Gradient descent is used for optimization of functions based on the derivations of them. The backpropagation algorithm, an efficient algorithm to compute a derivation of a function represented by a neural network, is introduced in Section \ref{subsection:backpropagation}. Finally, in Section \ref{subsection:convolutionalNN} convolutional neural networks are presented. A convolutional neural network represents a typical choice of the architecture of the neural network when it comes to the computer vision domain.

\section{Feedforward neural networks}
\label{subsection:FNN}
\input{chapters/background/sections/feedforward.tex}

\section{Gradient descent}
\label{subsection:gradient-descent}
\input{chapters/background/sections/gradient-descent.tex}

\section{Backpropagation}
\label{subsection:backpropagation}
\input{chapters/background/sections/backpropagation.tex}

\section{Convolutional neural networks}
\label{subsection:convolutionalNN}
\input{chapters/background/sections/convolutionalNN.tex}
