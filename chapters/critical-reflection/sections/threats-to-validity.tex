Skipping the Jacobian-based Dataset Augmentation step in transfer based approach.

One could argue that without the jacobian augmentation, the transfer based approach is incomplete algorithm. Given the hardware resources \footnote{Intel(R) Core(TM) i5-8500 2 CPU @ 3.00GHz, 16GB RAM, GeForce GTX 1080 8GB}, every time during the call for jacobian augmentation, process would get killed by the kernel due to memory consumption. The implementation of the jacobian augmentation that is used is the offical one provided by the author \cite{papernot2018cleverhans}. I tried to contact the author \footnote{https://github.com/tensorflow/cleverhans/issues/974}, but even he couldn't find a solution \footnote{https://stackoverflow.com/questions/54580105/memory-consumption-of-jacobian-dataset-augmentation/54718059}. However, I observed that when the number of potential classes is lower, the jacobian-based augmentation can be performed for the same network.