 \textit{Adversarial Saliency Maps} - maps which measure how much every pixel is important for an image to be classified as a specific class - are created. Based on them and the \textit{forward derivative} of the DNN, adversarial examples are crafted. The algorithm is validated against MNIST dataset \cite{datasetMNIST}, a digit-recognition task (0-9). The result is that using the JSMA algorithm, an attacker is able to craft a successful adversarial sample for every class.
