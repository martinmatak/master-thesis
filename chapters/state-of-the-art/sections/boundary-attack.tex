In every iteration of the attack, the image is changed a bit towards a class which will be in the image in the end, according to a human observer. After every change, the DNN is queried to check if the image is still adversarial, i.e. classified as a target label. If not, the change is reverted. In this way, they don't need any substitute neural network, but they do need a lot of queries to the targeted DNN. Authors compare this attack with CW attack on MNIST and CIFAR-10 and produce only a bit worse results, although this attack is treating a DNN as a black-box. This approach will be compared with the \textit{ensemble} approach in this thesis.
