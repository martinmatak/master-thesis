This approach for the black-box is completely different than those introduced in the Sections \ref{sec:transfer-based} and \ref{sec:ensemble-approach}. It has nothing to do with neither a substitute DNN nor transferability of the adversarial examples. 

The attack starts with an image of a targeted class and then, step by step, it changes it to an image of some other class while staying adversarial, i.e. classified as a target class by a DNN under the attack. In every iteration of the attack, the image is changed a bit towards a class which will be in the image in the end, at least according to a human observer. 

After every change, the targeted DNN is queried to check if the image is still adversarial, i.e. classified as a target label. If not, the change is reverted. In this way, the attacker doesn't need any substitute neural network. However, this attack comes at cost of number of queries to the targeted DNN. For the targeted attack, the authors needed around $10^4$ to get the adversarial example. The real world systems could notice such intensive querying of their APIs and detect the attack. On top of that, the attacker needs both: an image of the targeted class and an image of the class under which the human observer should classify it. That could be an obstacle when the number of classes is high because it can happen then that it's not easy to obtain the image of the particular class.

Authors compare this attack with white-box CW attack, introduced in the section \ref{sec:CW}, on the MNIST and the CIFAR-10 dataset and produce only a bit worse results, although this attack is treating a targeted DNN as a black-box.  For more information about this approach, please consult the original paper\cite{brendel2018decisionbased}.
