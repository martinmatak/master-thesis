This technique is used to attack the DNN in the black-box settings. The idea is to create a \textit{substitute} DNN which should be similar to the targeted DNN. Precise definition of the similarity is omitted here because it's not well defined, but the substitute DNN should solve the same task as the targeted DNN.

Adversarial images are crafted then for a substitute DNN using a white-box approach, for instance FGSM attack introduced in section \ref{sec:FGSM}. Created adversarial images are used then as adversarial images for a black-box DNN as well. The main idea is that the similar classifiers will have the similar boundaries for a specific class and therefore a same adversarial example should be adversarial for both networks. 

The dataset on which the substitute neural network is trained should be similar to the dataset on which the targeted neural network is trained. Ideally, that would be the same dataset, but the assumption is that an attacker doesn't have an access to that data. 

An attacker therefore generates a Synthetic Dataset. The attacker starts by querying the targeted DNN with several examples and obtains labels for them. Afterwards, the attacker expands the dataset using the Jacobian-based Dataset Augmentation and trains the substitute neural network. For more details how to generate the synthetic dataset, please consult the original paper  \cite{DBLP:journals/corr/PapernotMGJCS16}.

The Authors present good results for misclassification attack against MNIST dataset and GTSRD dataset \cite{datasetGTSRD}. Targeted misclassification is not presented in the paper.