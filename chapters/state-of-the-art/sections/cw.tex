% L norm introduction

To quantify similarity between two images, different distance metrics can be used. Quantification of similarity can be used when comparing how much an adversarial image is different from the original input. There are three widely-used distance metrics in the literature for generating adversarial examples, all of which are $L_p$ distances. The $L_p$ distance is written $||x - x'||_p$, where the $p$-norm is defined as

\[
||v||_p =( \sum_{i = 1}^{n} |v_i|^p)^{1/p}. 
\]

In other words, $L_0$ measures how many pixels are changed, $L_2$ measures standard euclidean distance and $L_\infty$ measures the maximum change to any of the coordinates. It is open for discussion which metric performs the best job in measuring the human perceptual of similarity, but neither of the $L_p$ metrics is optimal for that.

% end of L norm introduction

The authors \cite{DBLP:journals/corr/CarliniW16a} introduce three new attacks for the $L_0$, $L_2$, and $L_{ \infty }$ distance metrics. It is worth mentioning that their $L_0$ attack is the first published attack which can cause targeted misclassification on the ImageNet dataset. All three of them are based on optimization techniques.

In this thesis, $L_2$ is used in the attack and hence I explain it now.

Let $t$ be the target class, $\pmb Z$ be the output of the targeted DNN before the softmax layer with $\pmb Z_i$ as an output for the class $i$, $\kappa$ a parameter that controls the confidence with which the misclassification occurs, $c$ a constant value, and $\pmb x$ be the original input.
Given $\pmb x$ and the target class $t$, search for $\omega$ that solves

\[
argmin_{\omega} ||\frac{1}{2}*(tanh(\omega) + 1) - \pmb x||_2^2 + c * f(\frac{1}{2}(tanh(\omega) + 1))
\]
with $f$ defined as 
\[
f(\pmb x') = max(max\{\pmb Z(\pmb x ')_i : i \neq t\} - \pmb Z(x)_t, - \kappa).
\]

The unrestricted perturbation $\pmb \delta^*$ is then defined as 
\[
	\pmb \delta^*_i = \frac{1}{2} * (tanh ( \omega_i) + 1) - \pmb x_i
\]
and after converting it to the restricted perturbation $\pmb \delta$ (details in the original paper \cite{DBLP:journals/corr/CarliniW16a}), an adversarial example $\pmb x^*$ is produced as 
\[
\pmb x^* = \pmb x +\pmb  \delta.
\]

According to the authors, this attack is often much more effective (and never worse) than all the others presented in the literature. Attacks are evaluated on the three datasets: ImageNet, MNIST and CIFAR-10. They also report that the JSMA attack, an attack introduced in Section \ref{sec:JSMA}, is always failing on the ImageNet dataset due to memory complexity of the algorithm, i.e. dimensions of images in ImageNet dataset are too big for JSMA attack. This implies that the JSMA attack would not work in my thesis as well if an image of a person is too big. Reported results for the CW attack are showing 100\% success against all three datasets.







